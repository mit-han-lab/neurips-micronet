{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T05:40:43.260166Z",
     "start_time": "2019-10-12T05:40:43.253466Z"
    }
   },
   "outputs": [],
   "source": [
    "from u import *\n",
    "from ut import *\n",
    "from data import *\n",
    "import quantized_model\n",
    "from quantized_model import evaluate, get_net\n",
    "quantized_model.distiller_vs_explicit = 'explicit' # switch off using distiller, which we use during quantization aware training\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "num_bits = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T05:47:23.003937Z",
     "start_time": "2019-10-12T05:47:22.747377Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): AdaptiveEmbedding(\n",
       "    (layers): ModuleList(\n",
       "      (0): Embedding(3500, 256)\n",
       "      (1): Embedding(21500, 64)\n",
       "      (2): Embedding(242735, 4)\n",
       "    )\n",
       "    (projections): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=False)\n",
       "      (1): Linear(in_features=4, out_features=256, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout1): Dropout(p=0)\n",
       "  (layers): ModuleList(\n",
       "    (0): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (3): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (4): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (5): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (6): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "    (7): Decoder(\n",
       "      (ln1): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln1): ExplicitQuantize()\n",
       "      (qkv): Linear(in_features=256, out_features=576, bias=True)\n",
       "      (quant_qkv): ExplicitQuantize()\n",
       "      (quant_attn): ExplicitQuantize()\n",
       "      (quant_attnv): ExplicitQuantize()\n",
       "      (out): Linear(in_features=192, out_features=256, bias=False)\n",
       "      (dropout): Dropout(p=0)\n",
       "      (ln2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "      (quant_ln2): ExplicitQuantize()\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (1): ExplicitReLUQuantize()\n",
       "        (2): Dropout(p=0)\n",
       "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (4): ExplicitQuantize()\n",
       "        (5): Dropout(p=0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant): ExplicitQuantize()\n",
       "  (dropout2): Dropout(p=0)\n",
       "  (loss): ProjectedAdaptiveLogSoftmax(\n",
       "    (clusters): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=3500, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=21500, bias=True)\n",
       "      (2): Linear(in_features=4, out_features=242735, bias=True)\n",
       "    )\n",
       "    (projections): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=False)\n",
       "      (1): Linear(in_features=256, out_features=4, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in hyperparameters and paths, etc\n",
    "c = Config(Wiki / 'quant_aware,1', device='cuda:0', distributed=False).load()\n",
    "\n",
    "# load in data\n",
    "data_val = SequentialIterator(c, c.eval_batch, split='valid')\n",
    "data_test = SequentialIterator(c, c.eval_batch, split='test')\n",
    "\n",
    "# create network\n",
    "net = get_net(c)\n",
    "net.load_state_dict(\n",
    "    torch.load(c.res / 'models/model-1-processed.pth')\n",
    ")\n",
    "net = net.to(c.device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T05:47:29.434351Z",
     "start_time": "2019-10-12T05:47:24.723174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: {'loss': 3.536329746246338, 'perplexity': 34.34064871540981, 'time': 2.0}\n",
      "test: {'loss': 3.5540616512298584, 'perplexity': 34.95500458840018, 'time': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "net.cache_keys = net.cache_values = None\n",
    "print('validation:', evaluate(c, data_val, net)) \n",
    "\n",
    "# test\n",
    "net.cache_keys = net.cache_values = None\n",
    "print('test:', evaluate(c, data_test, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T05:47:29.477967Z",
     "start_time": "2019-10-12T05:47:29.436569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.layers.0.weight sparsity 0.1896\n",
      "embed.layers.1.weight sparsity 0.34571\n",
      "embed.layers.2.weight sparsity 0.40006\n",
      "embed.projections.0.weight sparsity 0.52264\n",
      "embed.projections.1.weight sparsity 0.00097656\n",
      "layers.0.pos_emb sparsity 0\n",
      "layers.0.qkv.weight sparsity 0.40005\n",
      "layers.0.qkv.bias sparsity 0.050347\n",
      "layers.0.out.weight sparsity 0.53571\n",
      "layers.0.fc.0.weight sparsity 0.36091\n",
      "layers.0.fc.0.bias sparsity 0\n",
      "layers.0.fc.3.weight sparsity 0.29874\n",
      "layers.0.fc.3.bias sparsity 0.035156\n",
      "layers.1.pos_emb sparsity 0\n",
      "layers.1.qkv.weight sparsity 0.50439\n",
      "layers.1.qkv.bias sparsity 0.032986\n",
      "layers.1.out.weight sparsity 0.724\n",
      "layers.1.fc.0.weight sparsity 0.45222\n",
      "layers.1.fc.0.bias sparsity 0\n",
      "layers.1.fc.3.weight sparsity 0.4392\n",
      "layers.1.fc.3.bias sparsity 0.03125\n",
      "layers.2.pos_emb sparsity 0\n",
      "layers.2.qkv.weight sparsity 0.41308\n",
      "layers.2.qkv.bias sparsity 0.059028\n",
      "layers.2.out.weight sparsity 0.64268\n",
      "layers.2.fc.0.weight sparsity 0.47834\n",
      "layers.2.fc.0.bias sparsity 0\n",
      "layers.2.fc.3.weight sparsity 0.56703\n",
      "layers.2.fc.3.bias sparsity 0\n",
      "layers.3.pos_emb sparsity 0\n",
      "layers.3.qkv.weight sparsity 0.52268\n",
      "layers.3.qkv.bias sparsity 0.043403\n",
      "layers.3.out.weight sparsity 0.7005\n",
      "layers.3.fc.0.weight sparsity 0.47834\n",
      "layers.3.fc.0.bias sparsity 0\n",
      "layers.3.fc.3.weight sparsity 0.59834\n",
      "layers.3.fc.3.bias sparsity 0.0078125\n",
      "layers.4.pos_emb sparsity 0\n",
      "layers.4.qkv.weight sparsity 0.49137\n",
      "layers.4.qkv.bias sparsity 0.034722\n",
      "layers.4.out.weight sparsity 0.65314\n",
      "layers.4.fc.0.weight sparsity 0.49137\n",
      "layers.4.fc.0.bias sparsity 0.0026042\n",
      "layers.4.fc.3.weight sparsity 0.52268\n",
      "layers.4.fc.3.bias sparsity 0.027344\n",
      "layers.5.pos_emb sparsity 0\n",
      "layers.5.qkv.weight sparsity 0.49137\n",
      "layers.5.qkv.bias sparsity 0.032986\n",
      "layers.5.out.weight sparsity 0.64268\n",
      "layers.5.fc.0.weight sparsity 0.49137\n",
      "layers.5.fc.0.bias sparsity 0.0026042\n",
      "layers.5.fc.3.weight sparsity 0.49137\n",
      "layers.5.fc.3.bias sparsity 0.0078125\n",
      "layers.6.pos_emb sparsity 0\n",
      "layers.6.qkv.weight sparsity 0.52268\n",
      "layers.6.qkv.bias sparsity 0.036458\n",
      "layers.6.out.weight sparsity 0.724\n",
      "layers.6.fc.0.weight sparsity 0.4392\n",
      "layers.6.fc.0.bias sparsity 0.0013021\n",
      "layers.6.fc.3.weight sparsity 0.4392\n",
      "layers.6.fc.3.bias sparsity 0.011719\n",
      "layers.7.pos_emb sparsity 0\n",
      "layers.7.qkv.weight sparsity 0.40005\n",
      "layers.7.qkv.bias sparsity 0.027778\n",
      "layers.7.out.weight sparsity 0.52268\n",
      "layers.7.fc.0.weight sparsity 0.29874\n",
      "layers.7.fc.0.bias sparsity 0.0065104\n",
      "layers.7.fc.3.weight sparsity 0.36091\n",
      "layers.7.fc.3.bias sparsity 0.019531\n",
      "loss.clusters.weight sparsity 0.011719\n",
      "loss.clusters.bias sparsity 0\n",
      "loss.layers.0.bias sparsity 0.0054286\n",
      "loss.layers.1.bias sparsity 0.96088\n",
      "loss.layers.2.bias sparsity 0.83063\n",
      "loss.projections.0.weight sparsity 0.40002\n",
      "loss.projections.1.weight sparsity 0.0058594\n",
      "\n",
      "nonzero params 4801977\n",
      "total params 8296023\n",
      "total sparsity 0.42117\n",
      "\n",
      "total param size 1350556.03125\n",
      "total mask size 259250.65625\n",
      "total quantization size 97.96875\n",
      "total size 1609904.65625\n"
     ]
    }
   ],
   "source": [
    "nonzero_params = 0\n",
    "total_params = 0\n",
    "\n",
    "mask_size = 0\n",
    "quantization_size = 0\n",
    "\n",
    "for k, p in net.state_dict().items():\n",
    "    if k in ['loss.layers.0.weight', 'loss.layers.1.weight', 'loss.layers.2.weight']: # shared with input embedding\n",
    "        continue\n",
    "    param_type = k.split('.')[-1]\n",
    "    if param_type == 'max_abs':\n",
    "        quantization_size += 1\n",
    "    elif param_type == 'inv_scale':\n",
    "        quantization_size += (32 - num_bits) / 32\n",
    "    elif param_type in ['weight', 'bias', 'pos_emb']: # masked params\n",
    "        if '.ln1.' in k or '.ln2.' in k: # ignore layernorm beta, gamma, can be fused into fc\n",
    "            continue\n",
    "        nz = from_torch((p != 0).sum())\n",
    "        total = p.numel()\n",
    "        nonzero_params += nz\n",
    "        total_params += total\n",
    "        mask_size += total / 32\n",
    "        print(k, 'sparsity %.5g' % (1 - nz / total))\n",
    "    elif param_type in ['cache_theta_inv_softplus', 'cache_lambda_inv_sigmoid']:\n",
    "        nonzero_params += p.numel()\n",
    "        total_params += p.numel()\n",
    "    else:\n",
    "        raise RuntimeError('Should not happen')\n",
    "print()\n",
    "print('nonzero params', nonzero_params)\n",
    "print('total params', total_params)\n",
    "print('total sparsity %.5g' % (1 - nonzero_params / total_params))\n",
    "\n",
    "print()\n",
    "param_size = nonzero_params * num_bits / 32\n",
    "print('total param size', param_size)\n",
    "print('total mask size', mask_size)\n",
    "print('total quantization size', quantization_size)\n",
    "total_size = param_size + mask_size + quantization_size\n",
    "print('total size', total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T05:56:30.477702Z",
     "start_time": "2019-10-12T05:56:30.433432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplications 8810476.55309913\n",
      "matrix multiplications after quantization 2477946.53055913\n",
      "additions 8802663.205372013\n",
      "others fp32 842247\n",
      "total 12122856.735931143\n"
     ]
    }
   ],
   "source": [
    "params = net.state_dict()\n",
    "densities = {}\n",
    "\n",
    "matmuls = 0\n",
    "adds = 0\n",
    "others = 0\n",
    "\n",
    "# collect densities\n",
    "for k, p in params.items():\n",
    "    param_type = k.split('.')[-1]\n",
    "    if param_type == 'max_abs': # TODO\n",
    "        pass\n",
    "    elif param_type == 'inv_scale':\n",
    "        pass\n",
    "    elif param_type in ['weight', 'bias']:\n",
    "        nz = from_torch((p != 0).sum())\n",
    "        total = p.numel()\n",
    "        densities[k] = nz / total\n",
    "densities = defaultdict(lambda:1)\n",
    "\n",
    "# input embedding\n",
    "token_bin_counts = np.array([198232, 35479, 11858])\n",
    "token_bin_fracs = token_bin_counts / token_bin_counts.sum()\n",
    "for i, p in enumerate(token_bin_fracs):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    embed_weight = params['embed.layers.%s.weight' % i] \n",
    "    proj_weight = params['embed.projections.%s.weight' % (i - 1)]\n",
    "    proj_density = densities['embed.projections.%s.weight' % (i - 1)]\n",
    "    \n",
    "    h2, h1 = proj_weight.shape\n",
    "    matmuls += h1 * h2 * p * proj_density\n",
    "    adds += (h1 - 1) * h2 * p * proj_density\n",
    "\n",
    "n_layers = 8\n",
    "    \n",
    "for i in range(n_layers):\n",
    "    layer_matmuls = 0\n",
    "    layer_adds = 0\n",
    "    \n",
    "    # layer norm 1\n",
    "    ln1 = 256 + 256 * 2 + 255 + 1 + 256 + 256\n",
    "    \n",
    "    # qkv fully connected layer\n",
    "    w_density = densities['layers.%s.qkv.weight' % i]\n",
    "    b_density = densities['layers.%s.qkv.bias' % i]\n",
    "    layer_matmuls += 256 * 192 * 3 * w_density\n",
    "    layer_adds += (256 - 1) * 192 * 3 * w_density + 192 * 3 * b_density\n",
    "    \n",
    "    # q * k\n",
    "    layer_matmuls += 24 * 8 * 97\n",
    "    layer_adds += (24 - 1) * 8 * 97\n",
    "    \n",
    "    # positional embedding\n",
    "    layer_matmuls += 24 * 97\n",
    "    layer_adds += (24 - 1) * 97 + 97\n",
    "    \n",
    "    # softmax\n",
    "    sm = 97 + 97 - 1 + 97\n",
    "    \n",
    "    # attn * v\n",
    "    layer_matmuls += 97 * 24 * 8\n",
    "    layer_adds += (97 - 1) * 24 * 8\n",
    "    \n",
    "    # out fully connected layer\n",
    "    w_density = densities['layers.%s.out.weight' % i]\n",
    "    layer_matmuls += 192 * 256 * w_density\n",
    "    layer_adds += (192 - 1) * 256 * w_density\n",
    "    \n",
    "    # residual\n",
    "    layer_adds += 256\n",
    "    \n",
    "    # layer norm 2\n",
    "    ln2 = ln1\n",
    "    \n",
    "    # FFN 1\n",
    "    w_density = densities['layers.%s.fc.0.weight' % i]\n",
    "    b_density = densities['layers.%s.fc.0.bias' % i]\n",
    "    layer_matmuls += 256 * 768 * w_density\n",
    "    layer_adds += (256 - 1) * 768 * w_density + 768 * b_density\n",
    "    \n",
    "    # ReLU\n",
    "    relu = 768\n",
    "    \n",
    "    w_density = densities['layers.%s.fc.3.weight' % i]\n",
    "    b_density = densities['layers.%s.fc.3.bias' % i]\n",
    "    layer_matmuls += 768 * 256 * w_density\n",
    "    layer_adds += (768 - 1) * 256 * w_density + 256 * b_density\n",
    "    \n",
    "    # residual\n",
    "    layer_adds += 256\n",
    "    \n",
    "    matmuls += layer_matmuls\n",
    "    adds += layer_adds\n",
    "    others += ln1 + sm + ln2 + relu\n",
    "\n",
    "# clusters\n",
    "w_density = densities['loss.clusters.weight']\n",
    "b_density = densities['loss.clusters.bias']\n",
    "matmuls += 256 * 2 * w_density\n",
    "adds += (256 - 1) * 2 * w_density + 2 * b_density\n",
    "\n",
    "# projections for bin 1 and 2\n",
    "for i in range(1, 3):\n",
    "    w_density = densities['loss.projections.%s.weight' % (i - 1)]\n",
    "    matmuls += {0: 256 * 64, 1: 256 * 4}[i - 1] * w_density\n",
    "    adds += {0: (256 - 1) * 64, 1: (256 - 1) * 4}[i - 1] * w_density\n",
    "\n",
    "for i in range(3):\n",
    "    w = params['loss.layers.%s.weight' % i]\n",
    "    w_density = densities['loss.layers.%s.weight' % i]\n",
    "    b_density = densities['loss.layers.%s.bias' % i]\n",
    "    \n",
    "    v, h = w.shape\n",
    "    matmuls += v * h * w_density\n",
    "    adds += (h - 1) * v * w_density + v * b_density\n",
    "sm = sum([x * 3 - 1 for x in (3502, 21500, 242735)])\n",
    "others += sm\n",
    "\n",
    "matmuls += 256 * 2000\n",
    "adds += (256 - 1) * 2000\n",
    "cache_sm = 2000 * 3 - 1\n",
    "others += cache_sm\n",
    "\n",
    "mm_quantized = matmuls * num_bits / 32\n",
    "\n",
    "print('matrix multiplications', matmuls)\n",
    "print('matrix multiplications after quantization', mm_quantized)\n",
    "print('additions', adds)\n",
    "print('others fp32', others)\n",
    "total_flops = mm_quantized + adds + others\n",
    "print('total', total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T05:57:45.475575Z",
     "start_time": "2019-10-12T05:57:45.468946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score 0.04824737751078976\n"
     ]
    }
   ],
   "source": [
    "print('final score', total_size / 159e6 + total_flops / 318e6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distiller",
   "language": "python",
   "name": "distiller"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
